{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Play Store App Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in an app title? \n",
    "\n",
    "### Can we predict the number of installs a given app has with just a name? \n",
    "\n",
    "### What other factors are relevant to an app's success, and how accurately can we predict an app's installs using basic features you can web scrape from the Google Play Store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What relationship does a title have on the overall success of an app? If you are a software developer with an awesome Google Play Store application, what words could you use in the title to help your app succeed? \n",
    "\n",
    "The phrase \"don't judge a book by it's cover\" is perhaps the least used advice in America. And why not? With the sheer amount of information, books, apps, and articles it's practically impossible to make an informed decision on every single candidate. So it seems reasonable to pick from a few that catch your attention, and in the world of mobile applications the only advertisement you may see is a few words and an icon. \n",
    "\n",
    "So how can we as developers best entice people to our applications, using the few characters afforded in an application name? That is what we are setting out to learn. Through statistical and machine learning models, we hope to learn about the correlation between words in an app's title and that app's success, specifically in terms of user downloads.\n",
    "\n",
    "Naming decisions aren't the only decisions app developers/marketers face though, so we will also explore the effect an app's category, size, and other attributes have on installs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is coming from Kaggle.com, it includes web-scraped features from the Google Play Store. Lavanya Gupta maintains the dataset, and last updated it a month ago, February 2019. Although there is a great deal of Apple datasets available, Google has a dynamic play store system that makes web scraping more challenging. But, with the data now available to us we can use it to gain insight into what Google Play Apps are like on a high level.\n",
    "\n",
    "This data contains information for apps last gathered in August of 2018. Here is an example of the data (after processing):\n",
    "\n",
    "<img src=\"img/rating_dist.png\" style=\"width: 7in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of features\n",
    "\n",
    "<img src=\"img/data_head.png\" style=\"width: 7in;\"/>\n",
    "\n",
    "We have a total of 9,360 ratings, and a mean rating of 4.191838 with a standard deviation of 0.515263. The individual ratings are integers in the form of 1 to 5 stars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/corrs.png\" style=\"width: 6in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "\n",
    "<img src=\"img/categories_dist.png\" style=\"width: 7in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 33 categories represented, and the three most popular apps (to develop) by far are Family, Game, and Productivity, each with over 750 apps in the play store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Installs (Ranges, not hard numbers)\n",
    "\n",
    "<img src=\"img/categories_dist_by_installs.png\" style=\"width: 7in;\"/>\n",
    "\n",
    "The biggest challenge with this dataset is that we do not have hard numbers for the number of installs for a given app. As an app becomes more popular, we lose precision as to how many people have downloaded that app. The way the data is presented in the app store, if it has 15,000 downloads the app will display that it has \"10,000+\" downloads. If it has 250,000 downloads the data may just represent this as \"100,000+\" downloads. This is probably for a variety of reasons, including user/developer privacy. But it does make regression and establishing numerical relationships challenging.\n",
    "\n",
    "We have decided to treat the installs feature as a \"minimum installs\" feature instead. So if any of our models predicts that an app would have say 15,000 installs, it would be more correct to say that it is likely the app would have at a minimum 10,000-15,000 installs. \n",
    "\n",
    "The reason we don't predict which range the app would fall in is because it would be for a computer to establish the order of significance of the features is \"100+\" better or worse than \"10,000+\"? And since the ranges aren't distributed evenly (We have a range of 0-50 and a range from 10,000 to 99,999 installs), we would need to indicate to our models the relative size of the ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Popular Words\n",
    "\n",
    "We used the NLTK library in Python in order to analyze the titles of our apps before we started our machine learning. We first used the library to tokenize each title. This was in order to break down each title into individual words we can process quickly. Once this was done we could use the stop_words module in order to filter out common words which most likely don't have much meaning in a title, such as \"the\" or \"a\". We also filtered out words that were actually just punctuation, such as a period or parenthases. \n",
    "\n",
    "Once this was done, we used the frequency distribution in order to observe the most common words used in titles, \"Free\" was the most common words used. Most likely app developers/marketers trying to signal to users that using their app has no barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"img/most_common_words.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/top_25_words_installs.png\" style=\"width: 7in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The original data had 9,367 apps described, but 7 of those had NA values (incomplete observations) so we dropped them from the set since our dataset was large enough to minimize the problems of dropping a few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at relationships of other features to installs, to get a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using feature selection, we found that the optimal dependent variables to include were reviews, days_since_update, size, rating, and price. Furthermore, we dropped features: app, type, last_updated, current_ver, and android_min_ver; as they provide no value when creating correlations. We also dropped install due to the fact that install should always pull a perfect correlation as it is the column of interest. Finally we dropped category/genre, as one they are the same variables and two it was reoperationalized as individual categories, i.e: FAMILY, TOOLS, etc. This was done in order for us to find the correlation and importance of whether an app had many installs due to being a specific category. The following visualization shows the levels of feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/correlations_with_installs.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning models, we used k-Nearest Neighbors, RandomForest, and XGboost. We chose these because k-Nearest Neighbors and RandomForest are fairly different models so we wanted to see the difference in how they would perform, and XGboost is known for it's efficiency and performance. K_Nearest Neighbors works by searching for the most similar points in the training data and using that to make its predictions. Random forest uses a collection of decision trees, which splits the data, seperating it until it reaches a \"desicion\" - our predictions.\n",
    "\n",
    "We used the five variables found through feature selection in each of the models with our independent variable being installations.\n",
    "\n",
    "The following three visualizations are line graphs of the predictions made by the model vs. the actual values, and are ordered from the model that performed the poorest to the model that performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/nkk_acc.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xgb_acc.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rfc_acc.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations of the mean accuracy error, mean squared error, and the accuracy score are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mae_3_models.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/MSE.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/acc_3_models.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, XGBoost and RandomForest had about the same accuracy score, .59 and .6 respectively, whereas k-Nearest Neighbors had half that, at about .35. XGBoost and RandomForest also had similar numbers for mean accuracy error and mean squared error, and again k-Nearest neighbors performed poorly with it's mean squared error being around 9.6 times that of the other two and it's mean accuracy error being about 3.5 times the other two models. The specific results are in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/scores.png\" style=\"width: 6in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Analysis - Machine Learning - Linear Regression\n",
    "\n",
    "The first step we took in analysing the relationship between the words in the title of an app and the app's installs was to \"Vectorize\" the words in the title. There are so many words in total, it's challenging for a computer to store all the information (which words are present in the title, and more importantly which are not). So rather than store the actual words, a column for each word is produced, and if a word is present in the title than the column for that work is marked with a \"1\", otherwise with a 0. There are many, many more zeros than ones most of the time, so in order to store all of the information Python uses a \"sparse\" array, which basically assumes that every column is a zero until the next instance of a one, this makes it much easier for our computers to work with.\n",
    "\n",
    "The next step would be to determine the \"term frequency\" of a given word, since two instances of a given term is much more significant in a 5 word sentence than in a 1000 word essay. This is done automatically for us in our pipeline with sklearn's TfidfTransformer. \n",
    "\n",
    "Once the \"app title\" data has gone through this process, we perform a machine learning algorithm in order to find a linear regression for any given title. We run a grid search in order to determine if we need to fit to an intercept or normalize the data, and then we train our model on 90% of our data. We can then test with the remaining 10% of apps, and the results of that are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/reg_perf.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model did a good job of reacting to the large install base of the most popular apps, it was off by far to much when it came to the vast majority of smaller user-base apps. This is represented by it's mean absolute error, which is a high 12,479,771 minimum installs. This is massive for our smaller apps, which may have as few as 10-10,000 users. But not as big of a deal for the larger apps which run in the billions of installs, minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Analysis - Machine Learning - Multinomial NB\n",
    "\n",
    "We go through the same process as before when in comes to vectorizing and running tf-idf on our data, but now we run a multinomial nb classification model. By using a classification model our model stayed within the ranges that the web-scraped data gave us. However because of the loss in precision, it reacts much worse to the large user-base apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mnb_perf.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model had a 27.7% accuracy score. Although it did a much better job predicting the smaller user-base apps (which is the majority of apps) it did not classify the larger apps, like those developed by Google, well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "\n",
    "https://www.kaggle.com/lava18/google-play-store-apps\n",
    "\n",
    "Github Repo:\n",
    "\n",
    "https://github.com/DanielColinaCuartin/INFO370-Final-Projects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
