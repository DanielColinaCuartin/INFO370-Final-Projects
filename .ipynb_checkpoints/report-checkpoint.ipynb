{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in an app title? \n",
    "\n",
    "### Can we predict the number of installs a given app has with just a name? \n",
    "\n",
    "### What other factors are relevant to an app's success, and how accurately can we predict an app's installs using basic features you can web scrape from the Google Play Store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Play Store App Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What relationship does a title have on the overall success of an app? If you are a software developer with an awesome Google Play Store application, what words could you use in the title to help your app succeed? \n",
    "\n",
    "The phrase \"don't judge a book by it's cover\" is perhaps the least used advice in America. And why not? With the sheer amount of information, books, apps, and articles it's practically impossible to make an informed decision on every single candidate. So it seems reasonable to pick from a few that catch your attention, and in the world of mobile applications the only advertisement you may see is a few words and an icon. \n",
    "\n",
    "So how can we as developers best entice people to our applications, using the few characters afforded in an application name? That is what we are setting out to learn. Through statistical and machine learning models, we hope to learn about the correlation between words in an app's title and that app's success, specifically in terms of user downloads.\n",
    "\n",
    "Naming decisions aren't the only decisions app developers/marketers face though, so we will also explore the effect an app's category, size, and other attributes have on installs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is coming from Kaggle.com, it includes web-scraped features from the Google Play Store. Lavanya Gupta maintains the dataset, and last updated it a month ago, February 2019. Although there is a great deal of Apple datasets available, Google has a dynamic play store system that makes web scraping more challenging. But, with the data now available to us we can use it to gain insight into what Google Play Apps are like on a high level.\n",
    "\n",
    "This data contains information for apps last gathered in August of 2018. Here is an example of the data (after processing):\n",
    "\n",
    "<img src=\"img/rating_dist.png\" style=\"width: 7in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of features\n",
    "\n",
    "<img src=\"img/data_head.png\" style=\"width: 7in;\"/>\n",
    "\n",
    "We have a total of 9,360 ratings, and a mean rating of 4.191838 with a standard deviation of 0.515263. The individual ratings are integers in the form of 1 to 5 stars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/corrs.png\" style=\"width: 6in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "\n",
    "<img src=\"img/categories_dist.png\" style=\"width: 7in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 33 categories represented, and the three most popular apps (to develop) by far are Family, Game, and Productivity, each with over 750 apps in the play store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Installs (Ranges, not hard numbers)\n",
    "\n",
    "<img src=\"img/categories_dist_by_installs.png\" style=\"width: 7in;\"/>\n",
    "\n",
    "The biggest challenge with this dataset is that we do not have hard numbers for the number of installs for a given app. As an app becomes more popular, we lose precision as to how many people have downloaded that app. The way the data is presented in the app store, if it has 15,000 downloads the app will display that it has \"10,000+\" downloads. If it has 250,000 downloads the data may just represent this as \"100,000+\" downloads. This is probably for a variety of reasons, including user/developer privacy. But it does make regression and establishing numerical relationships challenging.\n",
    "\n",
    "We have decided to treat the installs feature as a \"minimum installs\" feature instead. So if any of our models predicts that an app would have say 15,000 installs, it would be more correct to say that it is likely the app would have at a minimum 10,000-15,000 installs. \n",
    "\n",
    "The reason we don't predict which range the app would fall in is because it would be for a computer to establish the order of significance of the features is \"100+\" better or worse than \"10,000+\"? And since the ranges aren't distributed evenly (We have a range of 0-50 and a range from 10,000 to 99,999 installs), we would need to indicate to our models the relative size of the ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Popular Words\n",
    "\n",
    "We used the NLTK library in Python in order to analyze the titles of our apps before we started our machine learning. We first used the library to tokenize each title. This was in order to break down each title into individual words we can process quickly. Once this was done we could use the stop_words module in order to filter out common words which most likely don't have much meaning in a title, such as \"the\" or \"a\". We also filtered out words that were actually just punctuation, such as a period or parenthases. \n",
    "\n",
    "Once this was done, we used the frequency distribution in order to observe the most common words used in titles, \"Free\" was the most common words used. Most likely app developers/marketers trying to signal to users that using their app has no barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"img/most_common_words.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/top_25_words_installs.png\" style=\"width: 7in;\"/>"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The original data had 9,367 apps described, but 7 of those had NA values (incomplete observations) so we dropped them from the set since our dataset was large enough to minimize the problems of dropping a few rows."
=======
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The original data had 9,367 apps described, but 7 of those had NA values (incomplete observations) so we dropped them from the set since our dataset was large enough to minimize the problems of dropping a few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Using feature selection, we found that the optimal dependent variables to include were reviews, days_since_update, size, rating, and price. The following visualization shows the levels of feature importance. "
=======
    "##### Look at relationships of other features to installs, to get a baseline"
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/correlations_with_installs.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "For our machine learning models, we used k-Nearest Neighbors, RandomForest, and XGboost. We chose these because k-Nearest Neighbors and RandomForest are fairly different models so we wanted to see the difference in how they would perform, and XGboost is known for it's efficiency and performance.\n",
    "\n",
    "The following three visualizations are line graphs of the predictions made by the model vs. the actual values, and are ordered from the model that performed the poorest to the model that performed the best."
=======
=======
    "Although originally the applications' categories, the capitalized rows in the above graph, showed that the categories: family, tools and games had a huge difference compared to the rest, once we compare them to the other features the results fell rather short. Though categories seemingly played a huge impact previously, it is clear that a features reviews, days_since_update, size, rating, price, and content_rating all play a significantly larger impact in determining whether an application will have more or less installs. Of which; reviews, days_since_update, size, and rating appear to be the most important as we can see from the significant jump in importance between price and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> 8844bc0979e821c9ffdee374f0affc749237757a
    "<img src=\"img/mae_3_models.png\" style=\"width: 8in;\"/>"
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<img src=\"img/nkk_acc.png\" style=\"width: 10in;\"/>"
=======
    "<img src=\"img/acc_3_models.png\" style=\"width: 8in;\"/>"
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/xgb_acc.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rfc_acc.png\" style=\"width: 10in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations of the mean accuracy error, mean squared error, and the accuracy score are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mae_3_models.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<img src=\"img/MSE.png\" style=\"width: 8in;\"/>"
=======
    "After creating the model predictions, we quickly noticed that out of the three model predictions, the RandomForest model deviated largely. While KNearestNeighbors and XGboost provide quite similar results. This is interesting and apparent in the both the MAE and Accuracy which pulled about a 0.5 and 0.62 respectively in both RandomForest and XGboost. Meanwhile for the RandomForestModel the MAE score was 2.0 and the accuracy score was also significantly lower at less than 0.2. This indicates the lack of accuracy in addition to worst results for RandomForest."
>>>>>>> 8844bc0979e821c9ffdee374f0affc749237757a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "<img src=\"img/acc_3_models.png\" style=\"width: 8in;\"/>"
=======
=======
>>>>>>> 8844bc0979e821c9ffdee374f0affc749237757a
    "## Title Analysis - Machine Learning - Linear Regression\n",
    "\n",
    "The first step we took in analysing the relationship between the words in the title of an app and the app's installs was to \"Vectorize\" the words in the title. There are so many words in total, it's challenging for a computer to store all the information (which words are present in the title, and more importantly which are not). So rather than store the actual words, a column for each word is produced, and if a word is present in the title than the column for that work is marked with a \"1\", otherwise with a 0. There are many, many more zeros than ones most of the time, so in order to store all of the information Python uses a \"sparse\" array, which basically assumes that every column is a zero until the next instance of a one, this makes it much easier for our computers to work with.\n",
    "\n",
    "The next step would be to determine the \"term frequency\" of a given word, since two instances of a given term is much more significant in a 5 word sentence than in a 1000 word essay. This is done automatically for us in our pipeline with sklearn's TfidfTransformer. \n",
    "\n",
    "Once the \"app title\" data has gone through this process, we perform a machine learning algorithm in order to find a linear regression for any given title. We run a grid search in order to determine if we need to fit to an intercept or normalize the data, and then we train our model on 90% of our data. We can then test with the remaining 10% of apps, and the results of that are below."
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "As can be seen, XGBoost and RandomForest had about the same accuracy score, .59 and .6 respectively, whereas k-Nearest Neighbors had half that, at about .35. XGBoost and RandomForest also had similar numbers for mean accuracy error and mean squared error, and again k-Nearest neighbors performed poorly with it's mean squared error being around 9.6 times that of the other two and it's mean accuracy error being about 3.5 times the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9eae2e8910cd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-9eae2e8910cd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <img src=\"img/scores.png\" style=\"width: 8in;\"/>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"img/scores.png\" style=\"width: 8in;\"/>"
=======
    "<img src=\"img/reg_perf.png\" style=\"width: 8in;\"/>"
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "<img src=\"img/reg_perf.png\" style=\"width: 8in;\"/>"
=======
    "## Title Analysis - Machine Learning - Multinomial NB\n",
    "\n",
    "We go through the same process as before when in comes to vectorizing and running tf-idf on our data, but now we run a multinomial nb classification model. "
>>>>>>> d984d8dd6b25ccda036d032f40106d16e93ddd81
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/mnb_perf.png\" style=\"width: 8in;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "\n",
    "https://www.kaggle.com/lava18/google-play-store-apps\n",
    "\n",
    "Github Repo:\n",
    "\n",
    "https://github.com/DanielColinaCuartin/INFO370-Final-Projects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
